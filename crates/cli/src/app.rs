use std::{
    cell::RefCell,
    convert::TryFrom,
    io::{self, Write},
    rc::Rc,
    time::Instant,
};

use anyhow::{Context, Result};
use candle_core::{DType, Tensor};
use deepseek_ocr_config::{AppConfig, LocalFileSystem};
use deepseek_ocr_core::{
    inference::{
        build_prompt_tokens, compute_image_embeddings, normalize_text, prepare_vision_inputs,
        render_prompt,
    },
    model::{DeepseekOcrModel, GenerateOptions},
    runtime::{default_dtype_for_device, prepare_device_and_dtype},
};
use image::DynamicImage;
use tokenizers::Tokenizer;
use tracing::info;

use crate::{
    args::Args,
    bench,
    prompt::load_prompt,
    resources::{ensure_config_file, ensure_tokenizer_file, prepare_weights_path},
};

pub fn run(args: Args) -> Result<()> {
    let bench_enabled = args.bench || args.bench_output.is_some();
    let bench_session = bench::maybe_start(bench_enabled, args.bench_output.clone())?;

    let prompt_raw = load_prompt(&args)?;

    let fs = LocalFileSystem::new("deepseek-ocr");
    let (mut app_config, descriptor) = AppConfig::load_or_init(&fs, args.config.as_deref())?;
    app_config += &args;
    app_config.normalise(&fs)?;
    let resources = app_config.active_model_resources(&fs)?;

    info!(
        "Using configuration {} (active model `{}`)",
        descriptor.location.display_with(&fs)?,
        app_config.models.active
    );

    let config_path = ensure_config_file(&fs, &resources.config)?;
    let tokenizer_path = ensure_tokenizer_file(&fs, &resources.tokenizer)?;
    let weights_path = prepare_weights_path(&fs, &resources.weights)?;

    let (device, maybe_precision) =
        prepare_device_and_dtype(app_config.inference.device, app_config.inference.precision)?;
    let dtype = maybe_precision.unwrap_or_else(|| default_dtype_for_device(&device));

    info!(
        "Loading model `{}` (device={:?}, dtype={:?}) using config {}",
        app_config.models.active,
        device,
        dtype,
        config_path.display()
    );

    let load_start = Instant::now();
    let model = DeepseekOcrModel::load(
        Some(&config_path),
        Some(&weights_path),
        device.clone(),
        dtype,
    )
    .context("failed to load DeepSeek-OCR model")?;
    info!(
        "Model ready in {:.2?} (flash-attn: {}, weights={})",
        load_start.elapsed(),
        model.flash_attention_enabled(),
        weights_path.display()
    );

    let tokenizer = Tokenizer::from_file(&tokenizer_path).map_err(|err| {
        anyhow::anyhow!(
            "failed to load tokenizer from {}: {err}",
            tokenizer_path.display()
        )
    })?;

    let prompt_with_template = render_prompt(&app_config.inference.template, "", &prompt_raw)?;
    let image_slots = prompt_with_template.matches("<image>").count();
    anyhow::ensure!(
        image_slots == args.images.len(),
        "prompt includes {image_slots} <image> tokens but {} image paths were provided",
        args.images.len()
    );

    let images: Vec<DynamicImage> = args
        .images
        .iter()
        .map(|path| {
            image::open(path).with_context(|| format!("failed to open image at {}", path.display()))
        })
        .collect::<Result<Vec<_>>>()?;

    let owned_inputs = prepare_vision_inputs(
        &model,
        &images,
        app_config.inference.base_size,
        app_config.inference.image_size,
        app_config.inference.crop_mode,
    )?;
    let embeddings = compute_image_embeddings(&model, &owned_inputs)?;

    let (input_ids_vec, mask_vec) = build_prompt_tokens(
        &tokenizer,
        &prompt_with_template,
        &embeddings,
        &owned_inputs,
        app_config.inference.base_size,
        app_config.inference.image_size,
        app_config.inference.crop_mode,
    )?;

    info!(
        "Prompt prepared: {} tokens ({} image tokens)",
        input_ids_vec.len(),
        mask_vec.iter().filter(|&&b| b != 0).count()
    );

    let input_ids = Tensor::from_vec(
        input_ids_vec.clone(),
        (1, input_ids_vec.len()),
        model.device(),
    )?
    .to_dtype(DType::I64)?;
    let mask_tensor = Tensor::from_vec(mask_vec.clone(), (1, mask_vec.len()), model.device())?
        .to_dtype(DType::U8)?;

    let mut options = GenerateOptions::new(app_config.inference.max_new_tokens);
    options.images_seq_mask = Some(&mask_tensor);
    if !embeddings.is_empty() {
        options.image_embeddings = Some(embeddings.as_slice());
    }
    options.eos_token_id = model.language_model().config().eos_token_id;
    options.use_cache = app_config.inference.use_cache;

    let tokenizer_for_stream = tokenizer.clone();
    let progress_state = Rc::new(RefCell::new(0usize));
    let stream_state = Rc::clone(&progress_state);
    let stdout = Rc::new(RefCell::new(io::stdout()));
    let stdout_handle = Rc::clone(&stdout);
    let progress_callback = move |count: usize, ids: &[i64]| {
        let mut last = stream_state.borrow_mut();
        if count <= *last {
            return;
        }
        let new_tokens: Vec<u32> = ids[*last..count]
            .iter()
            .filter_map(|&id| u32::try_from(id).ok())
            .collect();
        if !new_tokens.is_empty() {
            if let Ok(decoded) = tokenizer_for_stream.decode(&new_tokens, true) {
                if !decoded.is_empty() {
                    let mut handle = stdout_handle.borrow_mut();
                    let _ = write!(handle, "{}", decoded);
                    let _ = handle.flush();
                }
            }
        }
        *last = count;
    };
    options.progress_callback = Some(&progress_callback);

    info!(
        "Starting generation with requested budget {} tokens",
        app_config.inference.max_new_tokens
    );
    info!("--- Generation start ---");
    let gen_start = Instant::now();
    let generated = model.generate(&input_ids, options)?;
    let elapsed = gen_start.elapsed();
    info!("--- Generation done in {:.2?} ---", elapsed);

    let generated_tokens = generated
        .to_vec2::<i64>()?
        .into_iter()
        .next()
        .unwrap_or_default();
    
    let token_ids: Vec<u32> = generated_tokens
        .iter()
        .filter_map(|&id| u32::try_from(id).ok())
        .collect();
    
    // 解码时跳过特殊token，获得更干净的输出
    let decoded = tokenizer
        .decode(&token_ids, true)
        .unwrap_or_default();
    
    let normalized = normalize_text(&decoded);
    info!("Normalized length: {} chars", normalized.len());
    info!("Final output:\n{normalized}");
    
    // 直接输出纯文本结果到标准输出（不含日志）
    println!("{}", normalized);

    if let Some(session) = bench_session {
        let report = session.finalize()?;
        bench::print_summary(&report);
    }

    Ok(())
}
